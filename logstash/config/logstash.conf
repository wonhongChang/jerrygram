input {
  beats {
    port => 5044
  }

  # Kafka input for real-time events
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["search-events", "user-events", "post-events"]
    group_id => "logstash-events"
    consumer_threads => 3
    decorate_events => true
    codec => "json"
  }

  # File inputs for application logs
  file {
    path => "/var/log/apps/dotnet-backend/*.log"
    start_position => "beginning"
    tags => ["dotnet-backend", "file-log"]
    codec => "json"
  }

  file {
    path => "/var/log/apps/java-backend/*.log"
    start_position => "beginning"
    tags => ["java-backend", "file-log"]
    codec => "json"
  }

  file {
    path => "/var/log/apps/recommend-service/*.log"
    start_position => "beginning"
    tags => ["recommend-service", "file-log"]
    codec => "json"
  }
}

filter {
  # Process .NET backend file logs (Serilog JSON format)
  if "dotnet-backend" in [tags] and "file-log" in [tags] {
    # Serilog already outputs JSON, fields are at root level
    # Extract nested Properties fields to root level for easier querying
    if [Properties] {
      ruby {
        code => "
          properties = event.get('Properties')
          if properties.is_a?(Hash)
            properties.each do |key, value|
              event.set(key, value)
            end
          end
        "
      }
    }

    mutate {
      add_field => {
        "service" => "dotnet-backend"
        "technology" => "aspnet-core"
        "log_source" => "file"
      }
      # Rename Serilog timestamp to @timestamp if needed
      rename => { "Timestamp" => "log_timestamp" }
      rename => { "Level" => "log_level" }
      rename => { "MessageTemplate" => "message_template" }
    }

    # Parse Serilog timestamp
    if [log_timestamp] {
      date {
        match => [ "log_timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }

    # Extract correlation and request tracking fields
    if [CorrelationId] {
      mutate { add_field => { "correlation_id" => "%{CorrelationId}" } }
    }
    if [RequestId] {
      mutate { add_field => { "request_id" => "%{RequestId}" } }
    }
    if [UserId] {
      mutate { add_field => { "user_id" => "%{UserId}" } }
    }
    if [SessionId] {
      mutate { add_field => { "session_id" => "%{SessionId}" } }
    }
  }

  # Process Java backend logs
  if "java-backend" in [tags] and "file-log" in [tags] {
    mutate {
      add_field => {
        "service" => "java-backend"
        "technology" => "spring-boot"
        "log_source" => "file"
      }
    }
  }

  # Process recommend service logs
  if "recommend-service" in [tags] and "file-log" in [tags] {
    mutate {
      add_field => {
        "service" => "recommend-service"
        "technology" => "nodejs"
        "log_source" => "file"
      }
    }
  }

  # Process Kafka events (already JSON parsed by codec)
  if [@metadata][kafka] {
    mutate {
      add_field => {
        "log_source" => "kafka"
        "kafka_topic" => "%{[@metadata][kafka][topic]}"
        "kafka_partition" => "%{[@metadata][kafka][partition]}"
        "kafka_offset" => "%{[@metadata][kafka][offset]}"
      }
    }

    # Parse event timestamp (ISO8601 format from .NET)
    if [timestamp] {
      date {
        match => [ "timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }

    # Map topic to service and event type
    if [@metadata][kafka][topic] == "search-events" {
      mutate {
        add_field => {
          "service" => "dotnet-backend"
          "event_category" => "search"
        }
      }
      # Enrich search events
      if [searchType] {
        mutate { add_field => { "search_type" => "%{searchType}" } }
      }
      if [searchTerm] {
        mutate { add_field => { "search_term" => "%{searchTerm}" } }
      }
    }
    else if [@metadata][kafka][topic] == "user-events" {
      mutate {
        add_field => {
          "service" => "java-backend"
          "event_category" => "user"
        }
      }
      if [eventType] {
        mutate { add_field => { "user_event_type" => "%{eventType}" } }
      }
    }
    else if [@metadata][kafka][topic] == "post-events" {
      mutate {
        add_field => {
          "service" => "java-backend"
          "event_category" => "post"
        }
      }
      if [eventType] {
        mutate { add_field => { "post_event_type" => "%{eventType}" } }
      }
    }

    # Normalize common event fields
    if [eventId] {
      mutate { add_field => { "event_id" => "%{eventId}" } }
    }
    if [correlationId] {
      mutate { add_field => { "correlation_id" => "%{correlationId}" } }
    }
    if [userId] {
      mutate { add_field => { "user_id" => "%{userId}" } }
    }
    if [sessionId] {
      mutate { add_field => { "session_id" => "%{sessionId}" } }
    }
    if [ipAddress] {
      mutate { add_field => { "client_ip" => "%{ipAddress}" } }
    }
  }

  # Add common platform field
  mutate {
    add_field => {
      "platform" => "jerrygram"
      "environment" => "${ENVIRONMENT:development}"
    }
  }

  # Clean up original fields to avoid duplication
  mutate {
    remove_field => [
      "host", "agent", "ecs", "input", "log",
      "Properties", "RenderedMessage"
    ]
  }
}

output {
  # Index by service and date for better organization
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "jerrygram-%{service}-%{+YYYY.MM.dd}"
    # Fallback index if service is not set
    document_id => "%{[@metadata][kafka][offset]}-%{[@metadata][kafka][partition]}"
  }

  # Conditional stdout for debugging (enable with LOGSTASH_DEBUG env var)
  if "${LOGSTASH_DEBUG:false}" == "true" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
}
